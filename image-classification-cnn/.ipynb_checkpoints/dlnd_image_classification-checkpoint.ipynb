{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 1000:\n",
      "Image - Min Value: 11 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123b1c7b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 1000\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # Using the min and max normalization\n",
    "    x = np.true_divide(x - np.min(x), np.max(x) - np.min(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    x2 = np.zeros((np.size(x), np.max(x) + 1))\n",
    "    x2[np.arange(np.size(x)), x] = 1\n",
    "    return x2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name='x')\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    y = tf.placeholder(tf.int32, shape=(None, n_classes), name='y')\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return keep_prob\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.shape.as_list()[3], conv_num_outputs], mean=0.0, stddev=0.08))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, weights, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    height = x_tensor.shape.as_list()[1]\n",
    "    width = x_tensor.shape.as_list()[2]\n",
    "    depth = x_tensor.shape.as_list()[3]\n",
    "    flattened_x_tensor = tf.reshape(x_tensor, [-1, height * width * depth])\n",
    "    return flattened_x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.shape.as_list()[1], num_outputs], mean=0.0, stddev=0.08))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    fully_connected_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    fully_connected_layer = tf.nn.relu(fully_connected_layer)\n",
    "    return fully_connected_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # Could you please explain me that why mean and stddev needs to be an essential part of initializing the weights?\n",
    "    # I am not sure as to why we need to do it?\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.shape.as_list()[1], num_outputs], mean=0.0, stddev=0.08))\n",
    "    biases = tf.Variable(tf.zeros(num_outputs))\n",
    "    output = tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    # print(x.shape) - shape of the image is 32 * 32 * 3\n",
    "    #     conv_layer = conv2d_maxpool(x, 10, (4,4), (2,2), (2,2), (1,1))\n",
    "    #     conv_layer_2 = conv2d_maxpool(conv_layer, 10, (4,4), (2,2), (2,2), (1,1))\n",
    "    # Because it is `SAME` padding for tensorflow so stride matters more\n",
    "    # 32, 32, 3 -> 8,8,64\n",
    "    first_conv_layer = conv2d_maxpool(x, 64, (4,4), (2,2), (2,2), (2,2))\n",
    "    first_conv_layer = tf.nn.dropout(first_conv_layer, keep_prob)\n",
    "    # 8,8,64 -> 4,4,128\n",
    "    second_conv_layer = conv2d_maxpool(first_conv_layer, 128, (2,2), (1,1), (5,5), (1,1)) \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    # [None, 2048]\n",
    "    flattened_layer = flatten(second_conv_layer)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fully_connected_1 = fully_conn(flattened_layer, 1024)\n",
    "    fully_connected_1 = tf.nn.dropout(fully_connected_1, keep_prob)\n",
    "    fully_connected_2 = fully_conn(fully_connected_1, 512)\n",
    "    \n",
    "    # Apply an Output Layer\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    # num_outputs=40 - as I saw from `otuput method`\n",
    "    logits = output(fully_connected_2, 10)\n",
    "    \n",
    "    # return output\n",
    "    # return logits\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    \n",
    "    print('Loss is: {} and validation accuracy is: {}'.format(loss, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss is: 2.2353482246398926 and validation accuracy is: 0.20759999752044678\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss is: 2.0176072120666504 and validation accuracy is: 0.2808000147342682\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss is: 1.9270248413085938 and validation accuracy is: 0.3449999988079071\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss is: 1.8872051239013672 and validation accuracy is: 0.384799987077713\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss is: 1.7890857458114624 and validation accuracy is: 0.42399999499320984\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss is: 1.7004108428955078 and validation accuracy is: 0.446399986743927\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss is: 1.6891212463378906 and validation accuracy is: 0.4480000138282776\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss is: 1.616560935974121 and validation accuracy is: 0.4544000029563904\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss is: 1.5097053050994873 and validation accuracy is: 0.4832000136375427\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss is: 1.471855878829956 and validation accuracy is: 0.4943999946117401\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss is: 1.4426589012145996 and validation accuracy is: 0.48240000009536743\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss is: 1.3885504007339478 and validation accuracy is: 0.49000000953674316\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss is: 1.3370945453643799 and validation accuracy is: 0.5076000094413757\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss is: 1.3452378511428833 and validation accuracy is: 0.5131999850273132\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss is: 1.2551796436309814 and validation accuracy is: 0.5163999795913696\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss is: 1.2725613117218018 and validation accuracy is: 0.5206000208854675\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss is: 1.163426399230957 and validation accuracy is: 0.5252000093460083\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss is: 1.1821177005767822 and validation accuracy is: 0.5249999761581421\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss is: 1.0990242958068848 and validation accuracy is: 0.5468000173568726\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss is: 1.1222865581512451 and validation accuracy is: 0.5212000012397766\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss is: 1.0925028324127197 and validation accuracy is: 0.527999997138977\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss is: 1.0159187316894531 and validation accuracy is: 0.5472000241279602\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss is: 1.0830377340316772 and validation accuracy is: 0.5475999712944031\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss is: 1.030625581741333 and validation accuracy is: 0.5504000186920166\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss is: 0.9459902048110962 and validation accuracy is: 0.5568000078201294\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss is: 0.9056519269943237 and validation accuracy is: 0.5680000185966492\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss is: 0.9826533198356628 and validation accuracy is: 0.5370000004768372\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss is: 0.8096836805343628 and validation accuracy is: 0.574999988079071\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss is: 0.9109326601028442 and validation accuracy is: 0.5473999977111816\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss is: 0.9012724757194519 and validation accuracy is: 0.571399986743927\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss is: 0.826879620552063 and validation accuracy is: 0.5795999765396118\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss is: 0.7388250231742859 and validation accuracy is: 0.5802000164985657\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss is: 0.7061449289321899 and validation accuracy is: 0.5722000002861023\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss is: 0.6925517320632935 and validation accuracy is: 0.5741999745368958\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss is: 0.7165371775627136 and validation accuracy is: 0.5824000239372253\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss is: 0.6953861713409424 and validation accuracy is: 0.5835999846458435\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss is: 0.6101018190383911 and validation accuracy is: 0.590399980545044\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss is: 0.6387816667556763 and validation accuracy is: 0.5813999772071838\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss is: 0.617214024066925 and validation accuracy is: 0.5911999940872192\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss is: 0.6431460380554199 and validation accuracy is: 0.5827999711036682\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss is: 0.5629870295524597 and validation accuracy is: 0.5920000076293945\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss is: 0.6612495183944702 and validation accuracy is: 0.5802000164985657\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss is: 0.5405323505401611 and validation accuracy is: 0.600600004196167\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss is: 0.5268589854240417 and validation accuracy is: 0.5920000076293945\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss is: 0.5412182211875916 and validation accuracy is: 0.5968000292778015\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss is: 0.565859317779541 and validation accuracy is: 0.5985999703407288\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss is: 0.5455783605575562 and validation accuracy is: 0.5997999906539917\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss is: 0.5773411989212036 and validation accuracy is: 0.5879999995231628\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss is: 0.4847787022590637 and validation accuracy is: 0.5992000102996826\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss is: 0.4459264874458313 and validation accuracy is: 0.6060000061988831\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss is: 0.406424343585968 and validation accuracy is: 0.607200026512146\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss is: 0.44243136048316956 and validation accuracy is: 0.6028000116348267\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss is: 0.4373645782470703 and validation accuracy is: 0.59579998254776\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss is: 0.4713314473628998 and validation accuracy is: 0.6065999865531921\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss is: 0.3720865845680237 and validation accuracy is: 0.609000027179718\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss is: 0.37453693151474 and validation accuracy is: 0.6011999845504761\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss is: 0.35759079456329346 and validation accuracy is: 0.6075999736785889\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss is: 0.3855935335159302 and validation accuracy is: 0.6092000007629395\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss is: 0.3538180887699127 and validation accuracy is: 0.6079999804496765\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss is: 0.40510788559913635 and validation accuracy is: 0.605400025844574\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss is: 0.3124304413795471 and validation accuracy is: 0.6191999912261963\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss is: 0.29792994260787964 and validation accuracy is: 0.621999979019165\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss is: 0.2558441162109375 and validation accuracy is: 0.6209999918937683\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss is: 0.31045451760292053 and validation accuracy is: 0.6119999885559082\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss is: 0.27591317892074585 and validation accuracy is: 0.620199978351593\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss is: 0.3266383409500122 and validation accuracy is: 0.6177999973297119\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss is: 0.2733853757381439 and validation accuracy is: 0.6078000068664551\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss is: 0.23190227150917053 and validation accuracy is: 0.6074000000953674\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss is: 0.24688605964183807 and validation accuracy is: 0.6187999844551086\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss is: 0.24441900849342346 and validation accuracy is: 0.6133999824523926\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss is: 0.24493546783924103 and validation accuracy is: 0.6194000244140625\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss is: 0.24632592499256134 and validation accuracy is: 0.6205999851226807\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss is: 0.21296970546245575 and validation accuracy is: 0.6191999912261963\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss is: 0.23396162688732147 and validation accuracy is: 0.6051999926567078\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss is: 0.19370412826538086 and validation accuracy is: 0.6176000237464905\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss is: 0.2523519992828369 and validation accuracy is: 0.6141999959945679\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss is: 0.19659361243247986 and validation accuracy is: 0.626800000667572\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss is: 0.23616933822631836 and validation accuracy is: 0.6087999939918518\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss is: 0.16974283754825592 and validation accuracy is: 0.6248000264167786\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss is: 0.18678393959999084 and validation accuracy is: 0.6122000217437744\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss is: 0.16611869633197784 and validation accuracy is: 0.6294000148773193\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss is: 0.19951657950878143 and validation accuracy is: 0.6154000163078308\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss is: 0.11570687592029572 and validation accuracy is: 0.6200000047683716\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss is: 0.191377192735672 and validation accuracy is: 0.6173999905586243\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss is: 0.1692623645067215 and validation accuracy is: 0.6255999803543091\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss is: 0.20940089225769043 and validation accuracy is: 0.6233999729156494\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss is: 0.1545279175043106 and validation accuracy is: 0.6309999823570251\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss is: 0.21942444145679474 and validation accuracy is: 0.6290000081062317\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss is: 0.1283682882785797 and validation accuracy is: 0.6169999837875366\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss is: 0.14966166019439697 and validation accuracy is: 0.6237999796867371\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss is: 0.1813153326511383 and validation accuracy is: 0.6299999952316284\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss is: 0.12062922865152359 and validation accuracy is: 0.6187999844551086\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss is: 0.14996783435344696 and validation accuracy is: 0.6277999877929688\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss is: 0.11183170974254608 and validation accuracy is: 0.6331999897956848\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss is: 0.10242494195699692 and validation accuracy is: 0.6345999836921692\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss is: 0.1025795191526413 and validation accuracy is: 0.6291999816894531\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss is: 0.11327522993087769 and validation accuracy is: 0.6304000020027161\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss is: 0.11478714644908905 and validation accuracy is: 0.6273999810218811\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss is: 0.09679926931858063 and validation accuracy is: 0.6218000054359436\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss is: 0.09242668002843857 and validation accuracy is: 0.635200023651123\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss is: 2.261946201324463 and validation accuracy is: 0.17020000517368317\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss is: 2.114410877227783 and validation accuracy is: 0.19460000097751617\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss is: 1.9145103693008423 and validation accuracy is: 0.2879999876022339\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss is: 1.8408241271972656 and validation accuracy is: 0.3465999960899353\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss is: 1.8591915369033813 and validation accuracy is: 0.3467999994754791\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss is: 1.9444124698638916 and validation accuracy is: 0.3747999966144562\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss is: 1.629880666732788 and validation accuracy is: 0.4018000066280365\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss is: 1.498392105102539 and validation accuracy is: 0.4293999969959259\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss is: 1.6637903451919556 and validation accuracy is: 0.42879998683929443\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss is: 1.7541717290878296 and validation accuracy is: 0.4020000100135803\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss is: 1.6727535724639893 and validation accuracy is: 0.45179998874664307\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss is: 1.5118029117584229 and validation accuracy is: 0.42660000920295715\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss is: 1.3631824254989624 and validation accuracy is: 0.4544000029563904\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss is: 1.516737699508667 and validation accuracy is: 0.4593999981880188\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss is: 1.7119636535644531 and validation accuracy is: 0.4221999943256378\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss is: 1.5039111375808716 and validation accuracy is: 0.4814000129699707\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss is: 1.4308134317398071 and validation accuracy is: 0.46480000019073486\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss is: 1.2756484746932983 and validation accuracy is: 0.46000000834465027\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss is: 1.4162685871124268 and validation accuracy is: 0.49540001153945923\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss is: 1.4521476030349731 and validation accuracy is: 0.47999998927116394\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss is: 1.3332008123397827 and validation accuracy is: 0.5238000154495239\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss is: 1.3081471920013428 and validation accuracy is: 0.48260000348091125\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss is: 1.2263542413711548 and validation accuracy is: 0.527999997138977\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss is: 1.225844383239746 and validation accuracy is: 0.5157999992370605\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss is: 1.346198320388794 and validation accuracy is: 0.5189999938011169\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss is: 1.2740466594696045 and validation accuracy is: 0.5266000032424927\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss is: 1.2389167547225952 and validation accuracy is: 0.5022000074386597\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss is: 1.1654859781265259 and validation accuracy is: 0.5392000079154968\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss is: 1.1788055896759033 and validation accuracy is: 0.5335999727249146\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss is: 1.3472509384155273 and validation accuracy is: 0.5126000046730042\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss is: 1.1850217580795288 and validation accuracy is: 0.5486000180244446\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss is: 1.1769949197769165 and validation accuracy is: 0.5537999868392944\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss is: 1.1424200534820557 and validation accuracy is: 0.5221999883651733\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss is: 1.1463388204574585 and validation accuracy is: 0.5368000268936157\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss is: 1.1893612146377563 and validation accuracy is: 0.5555999875068665\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss is: 1.173420786857605 and validation accuracy is: 0.5654000043869019\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss is: 1.1541874408721924 and validation accuracy is: 0.5577999949455261\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss is: 1.1076091527938843 and validation accuracy is: 0.5450000166893005\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss is: 1.2081146240234375 and validation accuracy is: 0.5551999807357788\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss is: 1.0962936878204346 and validation accuracy is: 0.5546000003814697\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss is: 1.080629825592041 and validation accuracy is: 0.5838000178337097\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss is: 1.1426349878311157 and validation accuracy is: 0.579200029373169\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss is: 0.9722687602043152 and validation accuracy is: 0.5622000098228455\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss is: 1.0114589929580688 and validation accuracy is: 0.5781999826431274\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss is: 1.0783560276031494 and validation accuracy is: 0.5774000287055969\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss is: 1.0556981563568115 and validation accuracy is: 0.5965999960899353\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss is: 1.0574090480804443 and validation accuracy is: 0.5950000286102295\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss is: 0.934840977191925 and validation accuracy is: 0.5866000056266785\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss is: 1.1207048892974854 and validation accuracy is: 0.5490000247955322\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss is: 1.0534509420394897 and validation accuracy is: 0.5738000273704529\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss is: 0.9598265886306763 and validation accuracy is: 0.6164000034332275\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss is: 1.0108498334884644 and validation accuracy is: 0.5982000231742859\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss is: 0.9343574643135071 and validation accuracy is: 0.5961999893188477\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss is: 0.954818844795227 and validation accuracy is: 0.6028000116348267\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss is: 1.1267592906951904 and validation accuracy is: 0.5655999779701233\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss is: 0.9744941592216492 and validation accuracy is: 0.6046000123023987\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss is: 1.0388991832733154 and validation accuracy is: 0.607200026512146\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss is: 0.8496519327163696 and validation accuracy is: 0.6010000109672546\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss is: 1.0132055282592773 and validation accuracy is: 0.600600004196167\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss is: 0.9838597178459167 and validation accuracy is: 0.6132000088691711\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss is: 0.9350072145462036 and validation accuracy is: 0.6128000020980835\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss is: 0.9783538579940796 and validation accuracy is: 0.6323999762535095\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss is: 0.870718777179718 and validation accuracy is: 0.6123999953269958\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss is: 0.8908429145812988 and validation accuracy is: 0.6371999979019165\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss is: 0.9691541790962219 and validation accuracy is: 0.5917999744415283\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss is: 0.8890417814254761 and validation accuracy is: 0.6273999810218811\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss is: 0.909620463848114 and validation accuracy is: 0.6236000061035156\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss is: 0.8175705075263977 and validation accuracy is: 0.6150000095367432\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss is: 0.921826958656311 and validation accuracy is: 0.6269999742507935\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss is: 0.8870975375175476 and validation accuracy is: 0.6299999952316284\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss is: 0.7776792049407959 and validation accuracy is: 0.6452000141143799\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss is: 0.8702880144119263 and validation accuracy is: 0.6398000121116638\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss is: 0.792079746723175 and validation accuracy is: 0.6335999965667725\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss is: 0.8400893211364746 and validation accuracy is: 0.6381999850273132\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss is: 0.8369606137275696 and validation accuracy is: 0.6290000081062317\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss is: 0.8637232780456543 and validation accuracy is: 0.6403999924659729\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss is: 0.8813918828964233 and validation accuracy is: 0.6420000195503235\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss is: 0.7794893980026245 and validation accuracy is: 0.6115999817848206\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss is: 0.8166189193725586 and validation accuracy is: 0.642799973487854\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss is: 0.8707079887390137 and validation accuracy is: 0.6388000249862671\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss is: 0.8047040104866028 and validation accuracy is: 0.6353999972343445\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss is: 0.8687527775764465 and validation accuracy is: 0.6503999829292297\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss is: 0.7673348188400269 and validation accuracy is: 0.6367999911308289\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss is: 0.8392258882522583 and validation accuracy is: 0.6439999938011169\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss is: 0.8261014819145203 and validation accuracy is: 0.6266000270843506\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss is: 0.8172353506088257 and validation accuracy is: 0.644599974155426\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss is: 0.9047639966011047 and validation accuracy is: 0.642799973487854\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss is: 0.7871605157852173 and validation accuracy is: 0.6218000054359436\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss is: 0.8171864748001099 and validation accuracy is: 0.6373999714851379\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss is: 0.793338418006897 and validation accuracy is: 0.6553999781608582\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss is: 0.7840531468391418 and validation accuracy is: 0.651199996471405\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss is: 0.7967758178710938 and validation accuracy is: 0.6636000275611877\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss is: 0.7384163737297058 and validation accuracy is: 0.6452000141143799\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss is: 0.7374408841133118 and validation accuracy is: 0.6656000018119812\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss is: 0.8047709465026855 and validation accuracy is: 0.647599995136261\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss is: 0.7463105916976929 and validation accuracy is: 0.6687999963760376\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss is: 0.8360565304756165 and validation accuracy is: 0.6743999719619751\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss is: 0.6713155508041382 and validation accuracy is: 0.6488000154495239\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss is: 0.6984308958053589 and validation accuracy is: 0.6574000120162964\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss is: 0.757588267326355 and validation accuracy is: 0.6552000045776367\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss is: 0.7597399950027466 and validation accuracy is: 0.66839998960495\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss is: 0.8308315277099609 and validation accuracy is: 0.6639999747276306\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss is: 0.6626924276351929 and validation accuracy is: 0.652999997138977\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss is: 0.6868282556533813 and validation accuracy is: 0.6783999800682068\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss is: 0.7512253522872925 and validation accuracy is: 0.6607999801635742\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss is: 0.695184588432312 and validation accuracy is: 0.6593999862670898\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss is: 0.733368992805481 and validation accuracy is: 0.6805999875068665\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss is: 0.672270655632019 and validation accuracy is: 0.6582000255584717\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss is: 0.6679372787475586 and validation accuracy is: 0.6776000261306763\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss is: 0.7020984888076782 and validation accuracy is: 0.6678000092506409\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss is: 0.6539896726608276 and validation accuracy is: 0.6754000186920166\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss is: 0.7114509344100952 and validation accuracy is: 0.6615999937057495\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss is: 0.6767246127128601 and validation accuracy is: 0.6517999768257141\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss is: 0.6587441563606262 and validation accuracy is: 0.6732000112533569\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss is: 0.7213032245635986 and validation accuracy is: 0.6790000200271606\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss is: 0.6911385655403137 and validation accuracy is: 0.6615999937057495\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss is: 0.7892469167709351 and validation accuracy is: 0.6718000173568726\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss is: 0.5949136018753052 and validation accuracy is: 0.6692000031471252\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss is: 0.6623119115829468 and validation accuracy is: 0.6733999848365784\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss is: 0.6306740045547485 and validation accuracy is: 0.6769999861717224\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss is: 0.7162719964981079 and validation accuracy is: 0.6729999780654907\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss is: 0.6966283917427063 and validation accuracy is: 0.6809999942779541\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss is: 0.6356136202812195 and validation accuracy is: 0.6705999970436096\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss is: 0.6385881304740906 and validation accuracy is: 0.6638000011444092\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss is: 0.7032573819160461 and validation accuracy is: 0.6836000084877014\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss is: 0.677597165107727 and validation accuracy is: 0.6761999726295471\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss is: 0.7560286521911621 and validation accuracy is: 0.6704000234603882\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss is: 0.6332167387008667 and validation accuracy is: 0.6808000206947327\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss is: 0.5838049054145813 and validation accuracy is: 0.6779999732971191\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss is: 0.7236641645431519 and validation accuracy is: 0.682200014591217\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss is: 0.6654821634292603 and validation accuracy is: 0.6863999962806702\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss is: 0.6761792302131653 and validation accuracy is: 0.6908000111579895\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss is: 0.5670263171195984 and validation accuracy is: 0.6895999908447266\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss is: 0.6791889071464539 and validation accuracy is: 0.6758000254631042\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss is: 0.6309489607810974 and validation accuracy is: 0.6887999773025513\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss is: 0.6326953768730164 and validation accuracy is: 0.6758000254631042\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss is: 0.6536856889724731 and validation accuracy is: 0.6790000200271606\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss is: 0.5672821998596191 and validation accuracy is: 0.6848000288009644\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss is: 0.6611295342445374 and validation accuracy is: 0.6772000193595886\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss is: 0.5906033515930176 and validation accuracy is: 0.6891999840736389\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss is: 0.6440060138702393 and validation accuracy is: 0.6877999901771545\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss is: 0.7279502153396606 and validation accuracy is: 0.6944000124931335\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss is: 0.5593991279602051 and validation accuracy is: 0.6754000186920166\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss is: 0.6322014927864075 and validation accuracy is: 0.6958000063896179\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss is: 0.595159113407135 and validation accuracy is: 0.676800012588501\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss is: 0.6557297110557556 and validation accuracy is: 0.696399986743927\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss is: 0.6865396499633789 and validation accuracy is: 0.6995999813079834\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss is: 0.5606955289840698 and validation accuracy is: 0.6877999901771545\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss is: 0.5825045704841614 and validation accuracy is: 0.6958000063896179\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss is: 0.6496343016624451 and validation accuracy is: 0.6772000193595886\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss is: 0.6795451045036316 and validation accuracy is: 0.692799985408783\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss is: 0.6511915922164917 and validation accuracy is: 0.692799985408783\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss is: 0.5406544208526611 and validation accuracy is: 0.6916000247001648\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss is: 0.5730959177017212 and validation accuracy is: 0.6840000152587891\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss is: 0.6326637864112854 and validation accuracy is: 0.6790000200271606\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss is: 0.6248305439949036 and validation accuracy is: 0.6958000063896179\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss is: 0.605989933013916 and validation accuracy is: 0.6912000179290771\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss is: 0.5475701689720154 and validation accuracy is: 0.6880000233650208\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss is: 0.5749459266662598 and validation accuracy is: 0.6940000057220459\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss is: 0.649772047996521 and validation accuracy is: 0.6913999915122986\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss is: 0.5975205302238464 and validation accuracy is: 0.703000009059906\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss is: 0.6037384271621704 and validation accuracy is: 0.7013999819755554\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss is: 0.5845733880996704 and validation accuracy is: 0.6922000050544739\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss is: 0.536701500415802 and validation accuracy is: 0.6836000084877014\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss is: 0.6171164512634277 and validation accuracy is: 0.7009999752044678\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss is: 0.6133466958999634 and validation accuracy is: 0.6970000267028809\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss is: 0.614563524723053 and validation accuracy is: 0.7003999948501587\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss is: 0.5308284163475037 and validation accuracy is: 0.6912000179290771\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss is: 0.5286372303962708 and validation accuracy is: 0.6901999711990356\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss is: 0.5841639637947083 and validation accuracy is: 0.6990000009536743\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss is: 0.582975447177887 and validation accuracy is: 0.6958000063896179\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss is: 0.6036059856414795 and validation accuracy is: 0.6988000273704529\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss is: 0.5281635522842407 and validation accuracy is: 0.6985999941825867\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss is: 0.5556817054748535 and validation accuracy is: 0.6836000084877014\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss is: 0.6092525720596313 and validation accuracy is: 0.6995999813079834\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss is: 0.576928973197937 and validation accuracy is: 0.6998000144958496\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss is: 0.5830584764480591 and validation accuracy is: 0.7125999927520752\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss is: 0.5401108860969543 and validation accuracy is: 0.6913999915122986\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss is: 0.5765241980552673 and validation accuracy is: 0.7009999752044678\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss is: 0.555519700050354 and validation accuracy is: 0.7074000239372253\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss is: 0.6087654232978821 and validation accuracy is: 0.7002000212669373\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss is: 0.6969386339187622 and validation accuracy is: 0.6955999732017517\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss is: 0.5197177529335022 and validation accuracy is: 0.7031999826431274\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss is: 0.5296452641487122 and validation accuracy is: 0.6923999786376953\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss is: 0.5314891934394836 and validation accuracy is: 0.7084000110626221\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss is: 0.6591037511825562 and validation accuracy is: 0.7084000110626221\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss is: 0.5937803387641907 and validation accuracy is: 0.7045999765396118\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss is: 0.4979552626609802 and validation accuracy is: 0.7059999704360962\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss is: 0.48326388001441956 and validation accuracy is: 0.6958000063896179\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss is: 0.5438388586044312 and validation accuracy is: 0.7099999785423279\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss is: 0.5670467019081116 and validation accuracy is: 0.7121999859809875\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss is: 0.5396421551704407 and validation accuracy is: 0.6901999711990356\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss is: 0.5073097348213196 and validation accuracy is: 0.7049999833106995\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss is: 0.5127109289169312 and validation accuracy is: 0.7089999914169312\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss is: 0.553331732749939 and validation accuracy is: 0.7143999934196472\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss is: 0.555773913860321 and validation accuracy is: 0.7084000110626221\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss is: 0.5872131586074829 and validation accuracy is: 0.7077999711036682\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss is: 0.5479788780212402 and validation accuracy is: 0.6984000205993652\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss is: 0.48546671867370605 and validation accuracy is: 0.6970000267028809\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss is: 0.5308495163917542 and validation accuracy is: 0.7098000049591064\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss is: 0.5447314381599426 and validation accuracy is: 0.7098000049591064\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss is: 0.6144372820854187 and validation accuracy is: 0.692799985408783\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss is: 0.4752442240715027 and validation accuracy is: 0.7075999975204468\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss is: 0.448943555355072 and validation accuracy is: 0.7129999995231628\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss is: 0.5256074666976929 and validation accuracy is: 0.7017999887466431\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss is: 0.5825809240341187 and validation accuracy is: 0.7110000252723694\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss is: 0.5947582125663757 and validation accuracy is: 0.703000009059906\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss is: 0.42023763060569763 and validation accuracy is: 0.7166000008583069\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss is: 0.46011942625045776 and validation accuracy is: 0.7085999846458435\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss is: 0.5139850974082947 and validation accuracy is: 0.7080000042915344\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss is: 0.5376571416854858 and validation accuracy is: 0.7110000252723694\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss is: 0.5827749371528625 and validation accuracy is: 0.7107999920845032\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss is: 0.4429965913295746 and validation accuracy is: 0.7031999826431274\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss is: 0.47555065155029297 and validation accuracy is: 0.7034000158309937\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss is: 0.5484004020690918 and validation accuracy is: 0.6990000009536743\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss is: 0.5195883512496948 and validation accuracy is: 0.7113999724388123\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss is: 0.5493301153182983 and validation accuracy is: 0.7196000218391418\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss is: 0.4648258686065674 and validation accuracy is: 0.7081999778747559\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss is: 0.47804251313209534 and validation accuracy is: 0.6976000070571899\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss is: 0.5270220637321472 and validation accuracy is: 0.7056000232696533\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss is: 0.5739866495132446 and validation accuracy is: 0.7102000117301941\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss is: 0.5476606488227844 and validation accuracy is: 0.7182000279426575\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss is: 0.41023024916648865 and validation accuracy is: 0.7129999995231628\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss is: 0.41247043013572693 and validation accuracy is: 0.7139999866485596\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss is: 0.5215853452682495 and validation accuracy is: 0.7085999846458435\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss is: 0.5435484647750854 and validation accuracy is: 0.704200029373169\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss is: 0.5645691156387329 and validation accuracy is: 0.7003999948501587\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss is: 0.4248577952384949 and validation accuracy is: 0.7098000049591064\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss is: 0.40330418944358826 and validation accuracy is: 0.7081999778747559\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss is: 0.503614068031311 and validation accuracy is: 0.7113999724388123\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss is: 0.5216256380081177 and validation accuracy is: 0.7106000185012817\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss is: 0.5918536186218262 and validation accuracy is: 0.7233999967575073\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss is: 0.39260247349739075 and validation accuracy is: 0.7175999879837036\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss is: 0.396772563457489 and validation accuracy is: 0.7174000144004822\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss is: 0.45424214005470276 and validation accuracy is: 0.7203999757766724\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss is: 0.509719967842102 and validation accuracy is: 0.7174000144004822\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss is: 0.6308817267417908 and validation accuracy is: 0.7003999948501587\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss is: 0.41076287627220154 and validation accuracy is: 0.722599983215332\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss is: 0.4449768662452698 and validation accuracy is: 0.722000002861023\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss is: 0.4548413157463074 and validation accuracy is: 0.7164000272750854\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss is: 0.506909191608429 and validation accuracy is: 0.6908000111579895\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss is: 0.5513015985488892 and validation accuracy is: 0.7107999920845032\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss is: 0.39166003465652466 and validation accuracy is: 0.7164000272750854\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss is: 0.4213944375514984 and validation accuracy is: 0.722599983215332\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss is: 0.47543254494667053 and validation accuracy is: 0.7057999968528748\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss is: 0.518041729927063 and validation accuracy is: 0.7131999731063843\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss is: 0.526479959487915 and validation accuracy is: 0.7099999785423279\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss is: 0.39338022470474243 and validation accuracy is: 0.7026000022888184\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss is: 0.4192224442958832 and validation accuracy is: 0.7211999893188477\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss is: 0.44913655519485474 and validation accuracy is: 0.7143999934196472\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss is: 0.4977121353149414 and validation accuracy is: 0.7214000225067139\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss is: 0.4981442093849182 and validation accuracy is: 0.7093999981880188\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss is: 0.3756965100765228 and validation accuracy is: 0.7110000252723694\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss is: 0.36613401770591736 and validation accuracy is: 0.7246000170707703\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss is: 0.46616584062576294 and validation accuracy is: 0.7148000001907349\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss is: 0.4821547567844391 and validation accuracy is: 0.7193999886512756\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss is: 0.47025734186172485 and validation accuracy is: 0.7170000076293945\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss is: 0.3596941828727722 and validation accuracy is: 0.7193999886512756\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss is: 0.37953072786331177 and validation accuracy is: 0.7221999764442444\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss is: 0.4333437979221344 and validation accuracy is: 0.7197999954223633\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss is: 0.4608596861362457 and validation accuracy is: 0.7160000205039978\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss is: 0.46749743819236755 and validation accuracy is: 0.7131999731063843\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss is: 0.3626084625720978 and validation accuracy is: 0.7200000286102295\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss is: 0.4107328951358795 and validation accuracy is: 0.7157999873161316\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss is: 0.4484415650367737 and validation accuracy is: 0.718999981880188\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss is: 0.5002433061599731 and validation accuracy is: 0.7265999913215637\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss is: 0.49185723066329956 and validation accuracy is: 0.7135999798774719\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss is: 0.35483670234680176 and validation accuracy is: 0.722599983215332\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss is: 0.4115305542945862 and validation accuracy is: 0.7128000259399414\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss is: 0.470388263463974 and validation accuracy is: 0.7257999777793884\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss is: 0.4727557301521301 and validation accuracy is: 0.7128000259399414\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss is: 0.45115524530410767 and validation accuracy is: 0.7134000062942505\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss is: 0.3576173186302185 and validation accuracy is: 0.723800003528595\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss is: 0.33936014771461487 and validation accuracy is: 0.7293999791145325\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss is: 0.4363706111907959 and validation accuracy is: 0.725600004196167\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss is: 0.42132410407066345 and validation accuracy is: 0.7324000000953674\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss is: 0.48044076561927795 and validation accuracy is: 0.7116000056266785\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss is: 0.36133480072021484 and validation accuracy is: 0.7088000178337097\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss is: 0.3431442677974701 and validation accuracy is: 0.7282000184059143\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss is: 0.38419944047927856 and validation accuracy is: 0.7268000245094299\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss is: 0.4577835202217102 and validation accuracy is: 0.7193999886512756\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss is: 0.41804131865501404 and validation accuracy is: 0.7250000238418579\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss is: 0.34595516324043274 and validation accuracy is: 0.717199981212616\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss is: 0.3856194317340851 and validation accuracy is: 0.7376000285148621\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss is: 0.43153682351112366 and validation accuracy is: 0.7185999751091003\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss is: 0.5016792416572571 and validation accuracy is: 0.7203999757766724\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss is: 0.43788671493530273 and validation accuracy is: 0.7239999771118164\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss is: 0.34789562225341797 and validation accuracy is: 0.724399983882904\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss is: 0.31364598870277405 and validation accuracy is: 0.7315999865531921\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss is: 0.4135745167732239 and validation accuracy is: 0.7297999858856201\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss is: 0.44211071729660034 and validation accuracy is: 0.7269999980926514\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss is: 0.48061102628707886 and validation accuracy is: 0.723800003528595\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss is: 0.36638307571411133 and validation accuracy is: 0.7297999858856201\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss is: 0.34746336936950684 and validation accuracy is: 0.7401999831199646\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss is: 0.42375969886779785 and validation accuracy is: 0.7182000279426575\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss is: 0.4204733967781067 and validation accuracy is: 0.7182000279426575\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss is: 0.49685588479042053 and validation accuracy is: 0.7131999731063843\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss is: 0.3715006709098816 and validation accuracy is: 0.7142000198364258\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss is: 0.3568408787250519 and validation accuracy is: 0.7310000061988831\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss is: 0.43069663643836975 and validation accuracy is: 0.7175999879837036\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss is: 0.4474516808986664 and validation accuracy is: 0.7319999933242798\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss is: 0.47256070375442505 and validation accuracy is: 0.7289999723434448\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss is: 0.3445797860622406 and validation accuracy is: 0.7215999960899353\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss is: 0.37361353635787964 and validation accuracy is: 0.7260000109672546\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss is: 0.3363514542579651 and validation accuracy is: 0.7188000082969666\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss is: 0.4185871183872223 and validation accuracy is: 0.7257999777793884\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss is: 0.41005292534828186 and validation accuracy is: 0.7228000164031982\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss is: 0.3189769685268402 and validation accuracy is: 0.7202000021934509\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss is: 0.3704926371574402 and validation accuracy is: 0.7307999730110168\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss is: 0.41610026359558105 and validation accuracy is: 0.7170000076293945\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss is: 0.36989927291870117 and validation accuracy is: 0.7328000068664551\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss is: 0.4703010022640228 and validation accuracy is: 0.7080000042915344\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss is: 0.3456262946128845 and validation accuracy is: 0.7242000102996826\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss is: 0.3702453076839447 and validation accuracy is: 0.7315999865531921\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss is: 0.3729248642921448 and validation accuracy is: 0.7206000089645386\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss is: 0.3898482918739319 and validation accuracy is: 0.7287999987602234\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss is: 0.4280354082584381 and validation accuracy is: 0.7179999947547913\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss is: 0.30699223279953003 and validation accuracy is: 0.7260000109672546\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss is: 0.33009618520736694 and validation accuracy is: 0.7360000014305115\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss is: 0.36057573556900024 and validation accuracy is: 0.7242000102996826\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss is: 0.39586830139160156 and validation accuracy is: 0.7289999723434448\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss is: 0.4580886960029602 and validation accuracy is: 0.7129999995231628\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss is: 0.3076859414577484 and validation accuracy is: 0.722599983215332\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss is: 0.3782716393470764 and validation accuracy is: 0.7242000102996826\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss is: 0.3300604522228241 and validation accuracy is: 0.7364000082015991\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss is: 0.4327206015586853 and validation accuracy is: 0.7343999743461609\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss is: 0.42218130826950073 and validation accuracy is: 0.7282000184059143\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss is: 0.37043556571006775 and validation accuracy is: 0.727400004863739\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss is: 0.348843514919281 and validation accuracy is: 0.730400025844574\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss is: 0.3354155421257019 and validation accuracy is: 0.7200000286102295\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss is: 0.4421921670436859 and validation accuracy is: 0.7275999784469604\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss is: 0.48856663703918457 and validation accuracy is: 0.7121999859809875\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss is: 0.2887447774410248 and validation accuracy is: 0.7142000198364258\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss is: 0.39791929721832275 and validation accuracy is: 0.7152000069618225\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss is: 0.3493417799472809 and validation accuracy is: 0.7161999940872192\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss is: 0.3433071970939636 and validation accuracy is: 0.7373999953269958\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss is: 0.48084574937820435 and validation accuracy is: 0.7089999914169312\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss is: 0.3228948712348938 and validation accuracy is: 0.7355999946594238\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss is: 0.37703973054885864 and validation accuracy is: 0.7315999865531921\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss is: 0.34241148829460144 and validation accuracy is: 0.7337999939918518\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss is: 0.37107232213020325 and validation accuracy is: 0.7287999987602234\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss is: 0.422799676656723 and validation accuracy is: 0.7261999845504761\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss is: 0.32918262481689453 and validation accuracy is: 0.7328000068664551\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss is: 0.3260480761528015 and validation accuracy is: 0.741599977016449\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss is: 0.357051283121109 and validation accuracy is: 0.7364000082015991\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss is: 0.3512383699417114 and validation accuracy is: 0.7390000224113464\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss is: 0.41599711775779724 and validation accuracy is: 0.7218000292778015\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss is: 0.31968173384666443 and validation accuracy is: 0.722000002861023\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss is: 0.33316683769226074 and validation accuracy is: 0.7253999710083008\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss is: 0.34676387906074524 and validation accuracy is: 0.7197999954223633\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss is: 0.41015440225601196 and validation accuracy is: 0.7379999756813049\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss is: 0.38967031240463257 and validation accuracy is: 0.7214000225067139\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss is: 0.29926422238349915 and validation accuracy is: 0.7293999791145325\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss is: 0.3169500231742859 and validation accuracy is: 0.7332000136375427\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss is: 0.3320302963256836 and validation accuracy is: 0.7337999939918518\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss is: 0.3445233702659607 and validation accuracy is: 0.7228000164031982\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss is: 0.3675251603126526 and validation accuracy is: 0.7128000259399414\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss is: 0.2811044156551361 and validation accuracy is: 0.7307999730110168\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss is: 0.30685845017433167 and validation accuracy is: 0.7271999716758728\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss is: 0.3364717364311218 and validation accuracy is: 0.7329999804496765\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss is: 0.4138382077217102 and validation accuracy is: 0.7342000007629395\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss is: 0.3850477635860443 and validation accuracy is: 0.7193999886512756\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss is: 0.3194809556007385 and validation accuracy is: 0.7232000231742859\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss is: 0.36608487367630005 and validation accuracy is: 0.7135999798774719\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss is: 0.35747915506362915 and validation accuracy is: 0.7157999873161316\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss is: 0.37251657247543335 and validation accuracy is: 0.7328000068664551\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss is: 0.37349992990493774 and validation accuracy is: 0.7268000245094299\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss is: 0.3096246123313904 and validation accuracy is: 0.7269999980926514\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss is: 0.3525378704071045 and validation accuracy is: 0.7184000015258789\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss is: 0.26042431592941284 and validation accuracy is: 0.7242000102996826\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss is: 0.3663407862186432 and validation accuracy is: 0.7354000210762024\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss is: 0.43542051315307617 and validation accuracy is: 0.7093999981880188\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss is: 0.33234700560569763 and validation accuracy is: 0.7390000224113464\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss is: 0.37180283665657043 and validation accuracy is: 0.7293999791145325\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss is: 0.29564496874809265 and validation accuracy is: 0.7350000143051147\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss is: 0.4167958199977875 and validation accuracy is: 0.7354000210762024\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss is: 0.3932923376560211 and validation accuracy is: 0.7365999817848206\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss is: 0.3020820617675781 and validation accuracy is: 0.7229999899864197\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss is: 0.3343469500541687 and validation accuracy is: 0.7287999987602234\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss is: 0.29772481322288513 and validation accuracy is: 0.7373999953269958\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss is: 0.40211719274520874 and validation accuracy is: 0.7242000102996826\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss is: 0.3818039894104004 and validation accuracy is: 0.7174000144004822\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss is: 0.29578256607055664 and validation accuracy is: 0.7235999703407288\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss is: 0.3230333924293518 and validation accuracy is: 0.7296000123023987\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss is: 0.29334455728530884 and validation accuracy is: 0.7355999946594238\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss is: 0.34955185651779175 and validation accuracy is: 0.7365999817848206\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss is: 0.3318105936050415 and validation accuracy is: 0.7214000225067139\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss is: 0.31966638565063477 and validation accuracy is: 0.730400025844574\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss is: 0.3093040883541107 and validation accuracy is: 0.734000027179718\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss is: 0.2457333505153656 and validation accuracy is: 0.7414000034332275\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss is: 0.40781012177467346 and validation accuracy is: 0.7383999824523926\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss is: 0.3836596608161926 and validation accuracy is: 0.7239999771118164\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss is: 0.30711132287979126 and validation accuracy is: 0.7211999893188477\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss is: 0.26482120156288147 and validation accuracy is: 0.7382000088691711\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss is: 0.25965791940689087 and validation accuracy is: 0.7351999878883362\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss is: 0.3716241717338562 and validation accuracy is: 0.7396000027656555\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss is: 0.31229937076568604 and validation accuracy is: 0.7278000116348267\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss is: 0.2786337435245514 and validation accuracy is: 0.7268000245094299\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss is: 0.30472350120544434 and validation accuracy is: 0.743399977684021\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss is: 0.28886622190475464 and validation accuracy is: 0.7210000157356262\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss is: 0.37369346618652344 and validation accuracy is: 0.7440000176429749\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss is: 0.4309085011482239 and validation accuracy is: 0.6966000199317932\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss is: 0.32683151960372925 and validation accuracy is: 0.7297999858856201\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss is: 0.2720671594142914 and validation accuracy is: 0.7468000054359436\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss is: 0.29390349984169006 and validation accuracy is: 0.7265999913215637\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss is: 0.3382417559623718 and validation accuracy is: 0.7319999933242798\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss is: 0.372412770986557 and validation accuracy is: 0.7289999723434448\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss is: 0.29597747325897217 and validation accuracy is: 0.7329999804496765\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss is: 0.3283819854259491 and validation accuracy is: 0.7247999906539917\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss is: 0.2560234069824219 and validation accuracy is: 0.7346000075340271\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss is: 0.3476349711418152 and validation accuracy is: 0.7444000244140625\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss is: 0.4244469702243805 and validation accuracy is: 0.7232000231742859\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss is: 0.25643450021743774 and validation accuracy is: 0.7332000136375427\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss is: 0.33863669633865356 and validation accuracy is: 0.7197999954223633\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss is: 0.22746840119361877 and validation accuracy is: 0.7487999796867371\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss is: 0.3417958915233612 and validation accuracy is: 0.7401999831199646\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss is: 0.34334108233451843 and validation accuracy is: 0.730400025844574\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss is: 0.2665356695652008 and validation accuracy is: 0.7333999872207642\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss is: 0.3010077178478241 and validation accuracy is: 0.7333999872207642\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss is: 0.2219579666852951 and validation accuracy is: 0.7315999865531921\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss is: 0.3401643633842468 and validation accuracy is: 0.7342000007629395\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss is: 0.30846816301345825 and validation accuracy is: 0.7293999791145325\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss is: 0.24866929650306702 and validation accuracy is: 0.7221999764442444\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss is: 0.26109015941619873 and validation accuracy is: 0.7409999966621399\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss is: 0.21349608898162842 and validation accuracy is: 0.7329999804496765\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss is: 0.32028716802597046 and validation accuracy is: 0.7325999736785889\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss is: 0.31774601340293884 and validation accuracy is: 0.7282000184059143\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss is: 0.27656617760658264 and validation accuracy is: 0.7337999939918518\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss is: 0.24089479446411133 and validation accuracy is: 0.7376000285148621\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss is: 0.2299608737230301 and validation accuracy is: 0.7337999939918518\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss is: 0.3629416823387146 and validation accuracy is: 0.7282000184059143\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss is: 0.33868956565856934 and validation accuracy is: 0.7157999873161316\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss is: 0.2532009780406952 and validation accuracy is: 0.72079998254776\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss is: 0.2560301423072815 and validation accuracy is: 0.7261999845504761\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss is: 0.23275575041770935 and validation accuracy is: 0.7246000170707703\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss is: 0.33956658840179443 and validation accuracy is: 0.7315999865531921\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss is: 0.27946338057518005 and validation accuracy is: 0.7218000292778015\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss is: 0.259795606136322 and validation accuracy is: 0.7157999873161316\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss is: 0.24606256186962128 and validation accuracy is: 0.7373999953269958\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss is: 0.20705914497375488 and validation accuracy is: 0.7283999919891357\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss is: 0.28516799211502075 and validation accuracy is: 0.7391999959945679\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss is: 0.2908935546875 and validation accuracy is: 0.7287999987602234\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss is: 0.2381400763988495 and validation accuracy is: 0.7382000088691711\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss is: 0.2674339711666107 and validation accuracy is: 0.7311999797821045\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss is: 0.25617003440856934 and validation accuracy is: 0.7246000170707703\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss is: 0.2997797429561615 and validation accuracy is: 0.7419999837875366\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss is: 0.259009450674057 and validation accuracy is: 0.7364000082015991\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss is: 0.2591498792171478 and validation accuracy is: 0.7218000292778015\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss is: 0.27681368589401245 and validation accuracy is: 0.7373999953269958\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss is: 0.24960041046142578 and validation accuracy is: 0.7310000061988831\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss is: 0.3136390149593353 and validation accuracy is: 0.7360000014305115\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss is: 0.3369468152523041 and validation accuracy is: 0.7203999757766724\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss is: 0.22962602972984314 and validation accuracy is: 0.7350000143051147\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss is: 0.2734847068786621 and validation accuracy is: 0.7419999837875366\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss is: 0.2490425854921341 and validation accuracy is: 0.723800003528595\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss is: 0.3052862286567688 and validation accuracy is: 0.7472000122070312\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss is: 0.3160454034805298 and validation accuracy is: 0.7156000137329102\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss is: 0.2867642343044281 and validation accuracy is: 0.7095999717712402\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss is: 0.3010043203830719 and validation accuracy is: 0.7275999784469604\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss is: 0.23435744643211365 and validation accuracy is: 0.72079998254776\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss is: 0.27405691146850586 and validation accuracy is: 0.7447999715805054\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss is: 0.3102797865867615 and validation accuracy is: 0.717199981212616\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss is: 0.21569609642028809 and validation accuracy is: 0.7224000096321106\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss is: 0.30258774757385254 and validation accuracy is: 0.7396000027656555\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss is: 0.215458944439888 and validation accuracy is: 0.73580002784729\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss is: 0.3451271653175354 and validation accuracy is: 0.7342000007629395\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss is: 0.2854169011116028 and validation accuracy is: 0.741599977016449\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss is: 0.21958892047405243 and validation accuracy is: 0.7289999723434448\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss is: 0.24750785529613495 and validation accuracy is: 0.725600004196167\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss is: 0.24434852600097656 and validation accuracy is: 0.7315999865531921\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss is: 0.25807636976242065 and validation accuracy is: 0.7436000108718872\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss is: 0.30706289410591125 and validation accuracy is: 0.7260000109672546\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss is: 0.23582538962364197 and validation accuracy is: 0.730400025844574\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss is: 0.22545984387397766 and validation accuracy is: 0.7486000061035156\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss is: 0.22160100936889648 and validation accuracy is: 0.7422000169754028\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss is: 0.271136075258255 and validation accuracy is: 0.7418000102043152\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss is: 0.35953211784362793 and validation accuracy is: 0.7247999906539917\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss is: 0.18849806487560272 and validation accuracy is: 0.7296000123023987\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss is: 0.2225549966096878 and validation accuracy is: 0.7462000250816345\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss is: 0.25083401799201965 and validation accuracy is: 0.7350000143051147\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss is: 0.3070920705795288 and validation accuracy is: 0.7447999715805054\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss is: 0.2900215983390808 and validation accuracy is: 0.7337999939918518\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss is: 0.18655958771705627 and validation accuracy is: 0.7459999918937683\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss is: 0.21291446685791016 and validation accuracy is: 0.7508000135421753\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss is: 0.20236483216285706 and validation accuracy is: 0.745199978351593\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss is: 0.29805392026901245 and validation accuracy is: 0.7494000196456909\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss is: 0.3881957530975342 and validation accuracy is: 0.724399983882904\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss is: 0.2187686264514923 and validation accuracy is: 0.7365999817848206\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss is: 0.2461816370487213 and validation accuracy is: 0.7447999715805054\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss is: 0.25041255354881287 and validation accuracy is: 0.7332000136375427\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss is: 0.2887718379497528 and validation accuracy is: 0.741599977016449\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss is: 0.28942060470581055 and validation accuracy is: 0.7454000115394592\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss is: 0.2519092857837677 and validation accuracy is: 0.7197999954223633\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss is: 0.23719820380210876 and validation accuracy is: 0.7382000088691711\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss is: 0.2084210216999054 and validation accuracy is: 0.7378000020980835\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss is: 0.3152259588241577 and validation accuracy is: 0.7355999946594238\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss is: 0.25580495595932007 and validation accuracy is: 0.7364000082015991\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss is: 0.23071765899658203 and validation accuracy is: 0.7296000123023987\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss is: 0.2239246666431427 and validation accuracy is: 0.7235999703407288\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss is: 0.20837469398975372 and validation accuracy is: 0.7310000061988831\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7237935126582279\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJFJwLCIiIiIS/X/p/HalGWAwDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c9c4b70>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
